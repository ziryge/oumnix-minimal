# Oumnix AI â€“ Proprietary Nonâ€‘Transformer Architecture

[![License](https://img.shields.io/badge/license-BSL--1.1-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.10%2B-blue.svg)](#)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.3%2B-orange.svg)](https://pytorch.org)
[![CUDA](https://img.shields.io/badge/CUDA-optional-lightgrey.svg)](#)
[![Status](https://img.shields.io/badge/status-experimental-yellow.svg)](#)
[![PRs](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](CONTRIBUTING.md)
[![Code of Conduct](https://img.shields.io/badge/Code%20of%20Conduct-Contributor%20Covenant-ff69b4.svg)](CODE_OF_CONDUCT.md)
[![Training](https://img.shields.io/badge/train-streaming%20recommended-blue.svg)](#)
[![Edition](https://img.shields.io/badge/Edition-Simple%20Edition-blueviolet.svg)](#editions)
[![YouTube](https://img.shields.io/badge/YouTube-Demo-red.svg)](https://www.youtube.com/watch?v=pOzOnSE1IAY)

A generative AI architecture that is an alternative to Transformers, focused on efficiency, longâ€‘term memory, and dynamic reasoning. This repository contains the full agent (CLI and Web), training pipelines (with a recommendation to use streaming training), and the memory, metacognition, and neurochemistry components.

Key architectural features
- Mixtureâ€‘ofâ€‘Operators per token: blends local/global attention, SSM, and convolution per time step.
- Bayesian Residuals (PFP â€“ propagation of uncertainty) in residual connections.
- WEAVE (weight factorization) + microâ€‘LoRA on critical projections for efficiency.
- Islet Injection (onâ€‘demand deltas â€“ disabled during training for stability).
- Retrievalâ€‘asâ€‘Attention (embedded RAG) via FAISS episodic memory.
- Dynamic depth (contextâ€‘adaptive steps and composition via metacognition/neurochemistry).
- Oumnix Cell (superposition â†’ local collapse) for discrete operator composition.

âˆâ€‘Window Memory
- Hotâ€‘KV (VRAM): recent states on GPU.
- Warmâ€‘KV (RAM): compression via Product Quantization (PQ) + Lowâ€‘Rank.
- Context tree + Teleport Attention for efficient contextual jumps.

Metacognition and Neurochemistry
- Metacognition: strategy selection, causal reasoning, analogy (see core/analogy_engine.py, core/causal_engine.py, core/metacognition.py).
- Advanced neurochemistry (neuro/advanced_chemistry.py): parameters modulated by DA/5â€‘HT/NE/ACh, altering temperature, depth, etc.

Mixed precision and FP8
- Optimized for RTX 4000 series (FP8 â€“ E4M3/E5M2), with fallback to FP16/AMP.


## Editions

### Simple Edition (Open Source)
This repository. Includes core Oumnix features such as modular operators, Infinity-Window memory, adaptive parameter control, and basic metacognition. Designed for research accessibility and consumer GPUs.

### Advanced Edition (Not Open Source)
A proprietary system that extends Oumnix far beyond the Simple Edition.
It introduces advanced runtime adaptability, encrypted full-state persistence, multi-level regulation, and mechanisms for long-term autonomous operation.
This edition is not publicly released.


## Context

Iâ€™m fully aware of the non-Transformer architectures being explored today:

- RWKV â€” RNN+Transformer hybrid with state evolution
- Mamba / SSMs â€” selective state spaces, efficient long-sequence modeling
- RetNet â€” retention mechanism (Microsoft)
- Hyena â€” convolutional sequence modeling without attention
- Mega â€” sparse attention with recurrence
- HyperMixer â€” MLP token-mixing with hypernets
- Differential Transformer â€” stable attention via signal subtraction

All of these are valuable directions.
But Oumnix is not any of them.

## What Makes Oumnix Different

- Mixture-of-Operators (MoOp): per-token routing across operators (beyond MoE)
- Bayesian Residuals: uncertainty-aware residuals for stable flow
- WEAVE + micro-LoRA: weight factorization with adaptive deltas
- Islet Injection: low-rank hyper-deltas infrastructure present (disabled by default in training)
- Retrieval-as-Attention: retrieval as a native pathway
- Dynamic Depth: discrete Neural-ODE style depth control
- Oumnix Cell: hypothesis superposition â†’ local collapse
- Infinity-Window: virtual infinite context memory (already available in the Simple Edition)

While others aim to replace attention or compress memory,
Oumnix explores neuro-inspired modularity, infinite-context memory, and operator-level adaptivity.


## Architecture Overview (ASCII)

```
                           +----------------------------------+
                           |          OumnixAI Wrapper        |
                           |  - Metacognition (basic)         |
                           |  - Neurochemistry modulation     |
User Text â”€â”€tokenizeâ”€â”€â–¶    |  - Infinity-Window integration   |
                           +---------â”¬------------------------+
                                     | memory_vectors (RAG)
                                     v
+--------------------------------------------------------------------+
|                    OumnixMinimal (Simple Edition)                  |
|                                                                    |
|   Token Embedding + Positional Embedding                           |
|            â”‚                                                       |
|            v                                                       |
|   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” |
|   |                OumnixMinimalBlock (repeated N)               | |
|   |                                                              | |
|   |  PFP (Bayesian Residuals): mu, sigma                         | |
|   |          â”‚                                                   | |
|   |          v                                                   | |
|   |  LayerNorm                                                   | |
|   |          â”‚                                                   | |
|   |          v                                                   | |
|   |  MoOp Gate (topâ€‘k perâ€‘token) â”€â”€â–º selects operators:          | |
|   |      â€¢ Local/Global Attention (RAG if memory_vectors)        | |
|   |      â€¢ SSM                                                   | |
|   |      â€¢ Convolution                                           | |
|   |          â”‚                                                   | |
|   |  Weighted combine + Residual add                             | |
|   |          â”‚                                                   | |
|   |  Oumnix Cell (odd layers): superposition â†’ local collapse    | |
|   |          â”‚                                                   | |
|   |  Depth Controller: earlyâ€‘exit at inference                   | |
|   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ |
|            â”‚                                                       |
|            v                                                       |
|        LayerNorm  â†’  LM Head  â†’  logits                            |
+--------------------------------------------------------------------+

Infinityâ€‘Window Memory (outside core):
  - Hotâ€‘KV (VRAM), Warmâ€‘KV (RAM, PQ+Lowâ€‘Rank), context tree + teleport.
  - Provides memory_vectors for Retrievalâ€‘asâ€‘Attention path.
```



## Beginners (Quickstart)


Demo video: 50Mâ€‘param model trained from loss ~8.0 to ~0.9 in ~13 minutes using this architecture â€” https://www.youtube.com/watch?v=pOzOnSE1IAY

1) Requirements
- Python 3.10+
- Optional CUDA (NVIDIA) â€“ recommended for training
- Dependencies: `pip install -r requirements.txt`

2) Run the agent (chat)
- Terminal (CLI):
```
python main.py --ui cli
```
- Web interface (Gradio):
```
python main.py --ui web
```
Tips:
- To load an automatically saved checkpoint: just run `main.py` â€“ it tries to detect `checkpo/`, `checkpoints_streaming/`, and `checkpoints/`.
- To force a specific checkpoint:
```
python main.py --checkpoint-path checkpoints/best_model.pt
```

3) Recommended training: Streaming
The streaming pipeline reads and tokenizes chunks directly from files in `datasets/`, reducing RAM/VRAM usage and enabling large datasets.
- Lightweight example (2â€“4 GB VRAM):
```
python train_streaming.py \
  --epochs 2 \
  --batch_size 2 \
  --chunk_size 500 \
  --max_length 256 \
  --use_amp \
  --log_interval 25 \
  --out_dir checkpoints_streaming
```
- On RTX 4000 GPUs, FP8 may be automatically detected; to force the flag in compatible environments:
```
python train_streaming.py --use_fp8
```
- To enable neurochemistry during streaming training:
```
python train_streaming.py --enable_neurochem
```

4) Expected data format
- Place your files in `datasets/` (accepted: .json, .jsonl, .txt)
- The reader supports JSON lines with keys `text` or `content`, lists, or conversation format under `conversations: [{role, content}]`.


#Advanced Users and Labs (controls)


A) Full agent runtime (main.py)
Flags:
- `--ui [cli|web]` â€“ UI. Default: `cli`.
- `--model-dim INT` â€“ Model dimensionality. Default: 768.
- `--layers INT` â€“ Number of layers. Default: 12.
- `--n-heads INT` â€“ Attention heads. Default: 12.
- `--checkpoint-path PATH` â€“ Checkpoint `.pt` path to load weights.
- `--align-config-from-checkpoint` â€“ Align dim/layers/heads to checkpoint when loading.
- `--load-state` â€“ Load previously saved complete state.
- `--state-dir DIR` â€“ Encrypted state directory. Default: `.ai_state`.
- `--password STR` â€“ Password for state encryption.
- `--no-neurochemistry` â€“ Disable neurochemistry.
- `--consolidation-interval INT` â€“ Consolidation interval in seconds.

B) Full training (train.py)
- Classic batch training that loads into RAM.
Flags:
- `--dim INT` (default 768)
- `--n_layers INT` (default 12)
- `--n_heads INT` (default 12)
- `--max_seq_len INT` (default 2048)
- `--epochs INT` (default 3)
- `--batch_size INT` (default 8)
- `--gradient_accumulation_steps INT` (default 4)
- `--lr FLOAT` (default 1e-4)
- `--weight_decay FLOAT` (default 0.01)
- `--max_grad_norm FLOAT` (default 1.0)
- `--use_amp` (mixed precision)
- `--num_workers INT` (default 2)
- `--log_interval INT` (default 100)
- `--out_dir DIR` (default checkpoints)

C) Simplified training (train_simple.py)
- Minimalist version for quick tests.
Flags:
- `--epochs INT` (default 2)
- `--batch_size INT` (default 4)
- `--lr FLOAT` (default 1e-4)
- `--dim INT` (default 256)
- `--n_layers INT` (default 4)
- `--n_heads INT` (default 4)
- `--use_amp`
- `--log_interval INT` (default 50)
- `--out_dir DIR` (default checkpoints_simple)
- `--max_samples INT` (default 2000)

D) Streaming training (train_streaming.py) â€“ recommended
Flags:
- `--epochs INT` (default 2)
- `--batch_size INT` (default 2)
- `--lr FLOAT` (default 5e-5)
- `--dim INT` (default 256)
- `--n_layers INT` (default 4)
- `--n_heads INT` (default 4)
- `--chunk_size INT` (default 500)
- `--max_chunks INT` (default 100) â€“ chunks per epoch (â‰¤0 streams all)
- `--max_length INT` (default 256)
- `--use_amp` â€“ mixed precision
- `--strip_tags` â€“ remove short tags <...> and normalize spaces
- `--enable_neurochem` â€“ enable neurochemistry during training
- `--use_fp8` â€“ attempt FP8 (useful on RTX 4000 with support)
- `--log_interval INT` (default 25)
- `--save_every_steps INT` (default 10000) â€“ incremental checkpoints
- `--out_dir DIR` (default checkpoints_streaming)
- `--resume` â€“ (reserved; resume support)

E) Programmatic config (OumnixAIConfig)
Parameters (core/oumnix_ai.py):
- vocabulary/dimension/layers/heads: `vocab_size`, `model_dim`, `n_layers`, `n_heads`
- memory: `hot_kv_size`, `warm_kv_windows`, `context_tree_fanout`
- metacognition: `max_reasoning_steps`, `strategy_beam_size`
- neurochemistry: `use_neurochemistry`, `neuro_update_frequency`
- consolidation/â€œdreamsâ€: `consolidation_interval`, `dream_duration`
- persistence/state: `auto_save_interval`, `state_dir`, `encryption_password`
- runtime: `max_sequence_length`, `batch_size`, `device`

Example:
```python
from core.oumnix_ai import OumnixAIConfig, create_oumnix_ai
from utils.tokenizer import tokenizer

config = OumnixAIConfig(
    vocab_size=tokenizer.vocab_size,
    model_dim=768,
    n_layers=12,
    n_heads=12,
    hot_kv_size=4096,
    use_neurochemistry=True,
    consolidation_interval=3600,
    state_dir='.ai_state'
)
ai = create_oumnix_ai(config)
ai.activate()
```

F) Checkpoints and state
- Trainers save `checkpoint_epoch_*.pt` and `best_model.pt` under `--out_dir`.
- `main.py` tries to autoâ€‘discover checkpoints under: `checkpoints_streaming/`, `checkpoints/` (legacy `checkpo/` is still scanned).
- Complete state (model + memory + metacognition + neuro + config) is saved/restored via `PersistenceManager` and `--load-state`/`--state-dir`.

G) Performance and VRAM
- Prefer `train_streaming.py` for large datasets and limited VRAM.
- Use `--use_amp` to reduce overhead; on RTX 4000 consider `--use_fp8` (or rely on automatic detection).
- Adjust `--chunk_size`, `--max_length`, and `--batch_size` based on your constraints.

H) Code layout (main folders)
- `core/`: minimal model (OumnixMinimal), AI wrapper, metacognition, loss, FP8 optimization.
- `memory/`: Infinity Window, advanced consolidator, persistence.
- `neuro/`: states and advanced chemistry.
- `ui/`: CLI/Web interfaces (simple and advanced versions).
- `utils/`: simple BPE tokenizer and tolerant dataset loader.

I) Notices and limits
- Some advanced mechanisms (Islet Injection, dynamic depth, Oumnix Cell collapse) are sensitive during training; Islet Injection is disabled by default while training.
- Embedded RAG depends on FAISS (CPU by default â€“ see `requirements.txt`).



## License


This project uses the Business Source License 1.1 (BSL 1.1). See `LICENSE` for full terms. Licensor: qrv0. Change Date: 2028. The final open license (Change License) will be defined by the Licensor by the Change Date.



## Credits

- Author: qrv0
- Acknowledgments: the openâ€‘source community and projects that inspired Mixtureâ€‘ofâ€‘Operators, SSM, and longâ€‘term memory ideas.
=======
# oumnix-minimal

> **A non-Transformer architecture.**
> This is the most basic form of oumnix.
> The full version already exists itâ€™s just not being shown yet.

---

## What is it?

oumnix-minimal is an experiment in training models that **do not follow the Transformer path**.
No papers. No replicas. Just a different line of exploration.

The images below show a real training run from scratch, absolute zero (not fine-tuning) on a 50M parameter model, trained on a notebook with an RTX 4060.
Nothing here is simulated.

<p align="center">
  <img src="Captura de imagem_20250824_215257.png" width="80%" />
</p>

<p align="center">
  <img src="Captura de imagem_20250824_215315.png" width="80%" />
</p>

[Watch on YouTube](https://www.youtube.com/watch?v=pOzOnSE1IAY)

---

## Status

* ğŸ”¹ This is the **minimal** version of oumnix.
* ğŸ”¹ The **full version already exists** but remains unrevealed.
* ğŸ”¹ No public code release (yet).

---

## Why share this now?

Itâ€™s not an announcement. Not a promise. Just an open record that **other paths exist** even outside the road paved by Transformers.
>>>>>>> 06f48c817bc2aa31c4f534ec76b15869c87cb933
